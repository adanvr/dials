% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/parameters.R
\name{weight}
\alias{weight}
\alias{text_parameters}
\alias{weight_scheme}
\alias{token}
\alias{max_times}
\alias{min_times}
\alias{max_tokens}
\title{Parameter objects related to text analysis.}
\usage{
weight(range = c(-10, 0), trans = log10_trans())

weight_scheme(values = c("raw count", "binary", "term frequency",
  "log normalization", "double normalization"))

token(values = c("words", "characters", "character_shingle", "lines",
  "ngrams", "paragraphs", "ptb", "regex", "sentences", "skip_ngrams",
  "tweets", "word_stems"))

max_times(range = c(1L, as.integer(10^5)), trans = NULL)

min_times(range = c(0L, 1000L), trans = NULL)

max_tokens(range = c(0L, as.integer(10^5)), trans = NULL)
}
\arguments{
\item{range}{A two-element vector holding the smallest and largest possible
values, respectively. If these cannot be set when the parameter is created,
\code{unknown()} can be used instead of a particular value. If a transformation
is specified, these should be in the \emph{transformed units}. The default is
an educated guess at the range of each parameter.}

\item{trans}{A \code{trans} object from the \code{scales} package, such as
\code{scales::log10_trans()} or \code{scales::reciprocal_trans()}. If not provided,
the default is used which matches the units used in \code{range}. If no
transformation, \code{NULL}.}

\item{values}{A vector of possible parameter values. For qualitative
parameters, character and logical defaults have been set, but can be modified
as needed.}
}
\value{
Each object is generated by either \code{new_quant_param()} or
\code{new_qual_param()}.
}
\description{
These are objects that can be used for modeling, especially in conjunction
with the \pkg{textrecipes} package.
}
\details{
These objects are pre-made parameter sets that are useful in a variety of
models.
\itemize{
\item \code{min_times()}, \code{max_times()}: frequency of word occurances for removal.
See \code{?step_tokenfilter}.
\item \code{max_tokens()}: the number of tokens that will be retained. See
\code{?step_tokenfilter}.
\item \code{weight()}: A parameter for "double normalization" when creating token
counts. See \code{?step_tf}.
\item \code{weight_scheme()}: the method for  term frequency calculations. Possible
values are: "binary", "raw count", "term frequency", "log normalization",
or "double normalization". See \code{?step_tf}.
\item \code{token()}: the type of token with possible values: "characters",
"character_shingle", "lines", "ngrams", "paragraphs", "ptb", "regex",
"sentences", "skip_ngrams", "tweets", "words", "word_stems". See
\code{?step_tokenize}
}
}
